# -*- coding: utf-8 -*-
"""Facial_Expression_Generation_Labeled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n5-JR9DnfHFEG_IU1QqY86CsfTLbRhtn
"""

!pip install torchvision

"""# **Facial Expression Generation Using GAN and Autoencoder**
Generate new facial expressions from neutral images using CelebA dataset. Use GAN and
Autoencoder architectures. Explore six hyperparameters such as latent vector size, learning rate,
number of discriminator/encoder layers, optimizer, dropout, and batch size.

# **Setup**
"""

# Mount Google Drive to save results
from google.colab import drive
drive.mount('/content/drive')

# Install dependencies
!pip install torchvision matplotlib

"""# **Imports and Parameters**"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.utils as vutils
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os

# Hyperparameters
batch_size = 64
image_size = 64
latent_dim = 64
lr = 0.0002
beta1 = 0.5
dropout_rate = 0.3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# **Load CelebA Dataset (Smiling only)**"""

import zipfile
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
zip_path = "/content/drive/MyDrive/celeba/img_align_celeba.zip"
attr_path = "/content/drive/MyDrive/celeba/list_attr_celeba.txt"

# Params
image_size = 64
batch_size = 64

# Load attributes (skip the header)
attr_df = pd.read_csv(attr_path, sep="\s+", skiprows=1)
attr_df.replace(-1, 0, inplace=True)  # make values 0 or 1

# Filter for smiling images only (col 31 is "Smiling")
smiling_df = attr_df[attr_df['Smiling'].isin([0, 1])]

# Define transform
transform = transforms.Compose([
    transforms.CenterCrop(178),
    transforms.Resize(image_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Custom Dataset
class CelebAZipDataset(Dataset):
    def __init__(self, zip_path, attr_df, transform=None):
        self.zip_path = zip_path
        self.attr_df = attr_df
        self.transform = transform
        self.image_names = attr_df.index.tolist()
        self.zip_file = None  # will be opened lazily

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        if self.zip_file is None:
            self.zip_file = zipfile.ZipFile(self.zip_path, 'r')

        img_name = f"img_align_celeba/{self.image_names[idx]}"
        attr = torch.tensor(self.attr_df.loc[self.image_names[idx]].values, dtype=torch.float32)

        with self.zip_file.open(img_name) as file:
            img = Image.open(file).convert('RGB')
            if self.transform:
                img = self.transform(img)

        return img, attr

# Dataset and DataLoader
dataset = CelebAZipDataset(zip_path, smiling_df, transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""# **Define Autoencoder**"""

class Encoder(nn.Module):
    def __init__(self, latent_dim=100):
        super(Encoder, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64*64*3, 1024),
            nn.ReLU(),
            nn.Linear(1024, latent_dim)
        )

    def forward(self, x):
        return self.model(x)

class Decoder(nn.Module):
    def __init__(self, latent_dim=100):
        super(Decoder, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 1024),
            nn.ReLU(),
            nn.Linear(1024, 64*64*3),
            nn.Sigmoid(),
            nn.Unflatten(1, (3, 64, 64))
        )

    def forward(self, x):
        return self.model(x)

"""# **Define Discriminator**"""

import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),      # 64x32x32
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1),    # 128x16x16
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 16 * 16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

"""# **Train the Model**"""

import os
import zipfile
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torchvision.utils as vutils
from google.colab import drive
import torchvision.models as models

# Mount Google Drive
drive.mount('/content/drive')

# Paths
data_root = '/content/drive/MyDrive/celeba'
zip_path = os.path.join(data_root, 'img_align_celeba.zip')
attr_path = os.path.join(data_root, 'list_attr_celeba.txt')
checkpoint_dir = os.path.join(data_root, 'checkpoints')
os.makedirs(checkpoint_dir, exist_ok=True)

# Load attributes
df_attr = pd.read_csv(attr_path, delim_whitespace=True, header=1)
df_attr = df_attr.replace(-1, 0)  # Convert -1 to 0

# Image transformation
image_size = 64
transform = transforms.Compose([
    transforms.CenterCrop(178),
    transforms.Resize(image_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Custom dataset to read from zip
class CelebAZipDataset(Dataset):
    def __init__(self, zip_path, attr_df, transform=None):
        self.zip_path = zip_path
        self.attr_df = attr_df
        self.transform = transform
        self.image_names = attr_df.index.tolist()
        self.zip_file = zipfile.ZipFile(self.zip_path, 'r')

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_name = f"img_align_celeba/{self.image_names[idx]}"
        attr = torch.tensor(self.attr_df.loc[self.image_names[idx]].values, dtype=torch.float32)

        with self.zip_file.open(img_name) as file:
            img = Image.open(file).convert('RGB')
            if self.transform:
                img = self.transform(img)

        return img, attr

# Filter for smiling attribute
df_smile = df_attr[df_attr['Smiling'].isin([0, 1])]

# Dataset and loader
dataset = CelebAZipDataset(zip_path, df_smile, transform)
batch_size = 64
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define model components (assumes Encoder, Decoder, Discriminator are defined)
encoder = Encoder().to(device)
decoder = Decoder().to(device)
discriminator = Discriminator().to(device)

# Loss functions and optimizers
recon_criterion = torch.nn.MSELoss()
gan_criterion = torch.nn.BCELoss()

lr = 0.0002
beta1 = 0.5
optimizer_G = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr, betas=(beta1, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))

# Learning rate schedulers
scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=3, gamma=0.5)
scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=3, gamma=0.5)

# Load perceptual loss model
vgg = models.vgg16(pretrained=True).features[:16].eval().to(device)
for param in vgg.parameters():
    param.requires_grad = False

def perceptual_loss(x, y):
    return torch.nn.functional.mse_loss(vgg(x), vgg(y))

# Optionally resume
checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_latest.pth')
start_epoch = 0
g_losses, d_losses = [], []

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    encoder.load_state_dict(checkpoint['encoder_state_dict'])
    decoder.load_state_dict(checkpoint['decoder_state_dict'])
    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
    g_losses = checkpoint['g_losses']
    d_losses = checkpoint['d_losses']
    start_epoch = checkpoint['epoch'] + 1
    print(f"âœ… Resumed from checkpoint (epoch {start_epoch})")

# Training loop
epochs = 8
for epoch in range(start_epoch, epochs):
    for i, (imgs, attr) in enumerate(dataloader):
        real_imgs = imgs.to(device)
        batch_size = real_imgs.size(0)

        # Labels with smoothing
        valid = torch.full((batch_size, 1), 0.9, device=device)
        fake = torch.full((batch_size, 1), 0.0, device=device)

        # Train Discriminator
        z = encoder(real_imgs)
        gen_imgs = decoder(z)

        real_loss = gan_criterion(discriminator(real_imgs), valid)
        fake_loss = gan_criterion(discriminator(gen_imgs.detach()), fake)
        d_loss = real_loss + fake_loss

        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # Train Generator + Encoder
        g_loss = gan_criterion(discriminator(gen_imgs), valid) + perceptual_loss(gen_imgs, real_imgs)

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

        # Save loss values
        g_losses.append(g_loss.item())
        d_losses.append(d_loss.item())

        if i % 100 == 0:
            print(f"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]")

    # Step schedulers
    scheduler_G.step()
    scheduler_D.step()

    # Save checkpoint
    torch.save({
        'epoch': epoch,
        'encoder_state_dict': encoder.state_dict(),
        'decoder_state_dict': decoder.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'optimizer_G_state_dict': optimizer_G.state_dict(),
        'optimizer_D_state_dict': optimizer_D.state_dict(),
        'g_losses': g_losses,
        'd_losses': d_losses
    }, checkpoint_path)
    print(f"ðŸ’¾ Checkpoint saved at epoch {epoch+1}")

    # Save model every 5 epochs
    if (epoch + 1) % 5 == 0:
        torch.save(decoder.state_dict(), f"/content/drive/MyDrive/generator_epoch_{epoch+1}.pth")

    # Show real vs generated
    with torch.no_grad():
        test_imgs = next(iter(dataloader))[0].to(device)[:8]
        z = encoder(test_imgs)
        recon = decoder(z)

        grid_real = vutils.make_grid(test_imgs, normalize=True)
        grid_fake = vutils.make_grid(recon, normalize=True)

        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1); plt.imshow(grid_real.permute(1, 2, 0).cpu()); plt.title("Real")
        plt.subplot(1, 2, 2); plt.imshow(grid_fake.permute(1, 2, 0).cpu()); plt.title("Generated")
        plt.show()

"""# **Generate and Visualize Output**"""

import matplotlib.pyplot as plt

encoder.eval()
decoder.eval()
with torch.no_grad():
    for i, (imgs, attr) in enumerate(dataloader):
        imgs = imgs.to(device)
        z = encoder(imgs)
        gen_imgs = decoder(z)
        break  # just one batch

# Unnormalize and plot
def show_images(real, fake):
    real = real * 0.5 + 0.5
    fake = fake * 0.5 + 0.5
    plt.figure(figsize=(12, 6))
    for i in range(8):
        plt.subplot(2, 8, i+1)
        plt.imshow(real[i].permute(1, 2, 0).cpu().numpy())
        plt.axis("off")
        plt.subplot(2, 8, i+9)
        plt.imshow(fake[i].permute(1, 2, 0).cpu().numpy())
        plt.axis("off")
    plt.show()

show_images(imgs, gen_imgs)

"""# **Plotting**"""

import matplotlib.pyplot as plt

# Cute pastel colors
g_color = '#ffb6c1'  # light pink
d_color = '#b39ddb'  # light purple

# Plot Generator and Discriminator Loss
plt.figure(figsize=(10, 5))
plt.title("Generator & Discriminator Loss During Training", fontsize=16, fontweight='bold', color='#ba68c8')

# Plot lines
plt.plot(g_losses, label="G Loss", color=g_color, linewidth=2)
plt.plot(d_losses, label="D Loss", color=d_color, linewidth=2)

# Labels & grid with bold styling
plt.xlabel("Iterations", fontsize=13, fontweight='bold', color='#6a1b9a')
plt.ylabel("Loss", fontsize=13, fontweight='bold', color='#6a1b9a')
plt.xticks(fontsize=11, fontweight='bold')
plt.yticks(fontsize=11, fontweight='bold')

# Legend with pastel touch
plt.legend(facecolor='#fce4ec', edgecolor='#ce93d8', fontsize=12)

# Backgrounds
plt.gca().set_facecolor('#fff0f5')         # Plot area
plt.gcf().patch.set_facecolor('#fce4ec')   # Figure background

plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""# **Train a Simple Classifier for Confusion Matrix & Accuracy**"""

# Create a basic classifier on top of the encoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from torch.utils.data import random_split
import numpy as np

# Collect features and labels
features = []
labels = []

encoder.eval()
with torch.no_grad():
    for imgs, attr in dataloader:
        imgs = imgs.to(device)
        z = encoder(imgs)
        features.append(z.cpu())
        labels.append(attr[:, 31])  # Smiling attribute

X = torch.cat(features).numpy()
y = torch.cat(labels).numpy()

# Train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train logistic regression
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"Final Accuracy: {acc:.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Smiling", "Smiling"])

# Plot with a purple-pink colormap
fig, ax = plt.subplots(figsize=(6, 6))
disp.plot(
    cmap=plt.cm.Purples,   # Try 'Purples', or 'pink' if using a custom colormap
    ax=ax,
    colorbar=False
)

# Customizing background and text colors
ax.set_title("ðŸ’œ Confusion Matrix", fontsize=14, color='#6a1b9a')
fig.patch.set_facecolor('#fce4ec')  # Light pink background
ax.set_facecolor('#f8bbd0')         # Softer pink plot background
plt.grid(False)
plt.tight_layout()
plt.show()

"""# **Hyperparameter Performance Summary (Manual Input)**"""

# Example results dictionary (fill in with your actual experiments)
results = {
    "Run 1 (lr=0.0002, latent_dim=64)": {"accuracy": 0.82, "g_loss": 1.03, "d_loss": 0.58},
    "Run 2 (lr=0.0001, latent_dim=128)": {"accuracy": 0.85, "g_loss": 0.91, "d_loss": 0.61},
    "Run 3 (lr=0.0003, latent_dim=32)": {"accuracy": 0.79, "g_loss": 1.10, "d_loss": 0.55},
}

# Display
for run, metrics in results.items():
    print(f"{run}:")
    for k, v in metrics.items():
        print(f"  {k}: {v}")
    print()

"""# **Compare Results**"""

import pandas as pd
import matplotlib.pyplot as plt

# Example results (you can replace this with your actual 'results' variable)
# results = {'Model A': [0.85, 0.88], 'Model B': [0.83, 0.86]}

# Convert results to DataFrame and transpose
df = pd.DataFrame(results).T

# Define pastel pink colors for bars
pastel_pinks = ['#f8bbd0', '#f48fb1', '#f06292', '#ec407a', '#e91e63']

# Set background and style
plt.style.use('ggplot')
fig, ax = plt.subplots(figsize=(12, 6))
fig.patch.set_facecolor('#ffe4ec')        # Figure background
ax.set_facecolor('#fff0f5')               # Plot area background

# Plot
df.plot(kind='bar', ax=ax, color=pastel_pinks[:len(df.columns)])
ax.set_title("Comparison of Hyperparameter Settings", fontsize=14, color='#c2185b')
ax.set_ylabel("Score", fontsize=12)
plt.xticks(rotation=45, ha='right')
ax.grid(True, linestyle='--', alpha=0.6)
ax.legend(loc='upper right', facecolor='#ffe4ec', edgecolor='gray')
plt.tight_layout()
plt.show()

"""# **ðŸ“¥ Input**
Dataset: CelebA (automatically downloaded)

Input: Neutral-face images (filtered using attribute index 31)

# **ðŸ“¤ Output**
Images with smiling expression (or close to that) generated by the autoencoder + GAN
"""

import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import os
import pandas as pd

# Define emotion categories
emotion_classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Load pretrained ResNet18 and modify final layer for 7-class FER
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 7)

# Load trained weights (Replace with actual checkpoint path if available)
# model.load_state_dict(torch.load('fer_resnet18.pth', map_location='cpu'))

model.eval()

import os
import zipfile
import pandas as pd
from PIL import Image
import torch
import torchvision.transforms as transforms

# Load your pre-trained model here
# Example:
# from your_model_file import load_model
# model = load_model()
model.eval()  # Ensure model is in evaluation mode

# Define emotion classes as per your model's output
emotion_classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Path to CelebA zipped image dataset
celeba_zip_path = '/content/drive/MyDrive/celeba/img_align_celeba.zip'
# Path where images are extracted inside the zip
extracted_path = '/content/celeba_images/img_align_celeba'

output_csv = 'celeba_emotion_labels.csv'

# Unzip CelebA if not already done
if not os.path.exists(extracted_path):
    with zipfile.ZipFile(celeba_zip_path, 'r') as zip_ref:
        zip_ref.extractall(extracted_path)
    print(f"Extracted images to {extracted_path}")
else:
    print(f"Images already extracted at {extracted_path}")

# Define image preprocessing transformation
transform = transforms.Compose([
    transforms.Resize((48, 48)),               # Resize to FER-2013 input size
    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)     # Normalize to [-1, 1]
])

# Emotion prediction loop
predictions = []

with torch.no_grad():
    image_files = sorted(os.listdir(extracted_path))[:1000]  # Limit to 1000 images
    for img_name in image_files:
        try:
            img_path = os.path.join(extracted_path, img_name)
            image = Image.open(img_path).convert("RGB")
            image = transform(image).unsqueeze(0)  # Add batch dimension
            output = model(image)
            pred_class = torch.argmax(output, dim=1).item()
            predictions.append((img_name, emotion_classes[pred_class]))
        except Exception as e:
            print(f"Error processing {img_name}: {e}")

# Save predictions to CSV
df = pd.DataFrame(predictions, columns=["image", "predicted_emotion"])
df.to_csv(output_csv, index=False)
print(f"Saved emotion labels to {output_csv}")